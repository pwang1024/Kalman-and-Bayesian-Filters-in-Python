{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](./table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The g-h Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the book\n",
    "import book_format\n",
    "book_format.set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, be sure you understand how to use [Jupyter Notebooks](http://jupyter.org/), and are familiar with the [SciPy](https://scipy.org), [NumPy](http://www.numpy.org/), and [Matplotlib](https://matplotlib.org/) packages, as they are used throughout this book. The Preface contains an introduction to these packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Intuition via Thought Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we live in a world without scales - the devices you stand on to weigh yourself. One day at work a co-worker comes running up to you and announces her invention of a 'scale' to you. After she explains, you eagerly stand on it and announce the results: \"172 lbs\". You are ecstatic - for the first time in your life you know what you weigh. More importantly, dollar signs dance in your eyes as you imagine selling this device to weight loss clinics across the world! This is fantastic!\n",
    "\n",
    "Another co-worker hears the commotion and comes over to find out what has you so excited. You explain the invention and once again step onto the scale, and proudly proclaim the result: \"161 lbs.\" And then you hesitate, confused.\n",
    "\n",
    "\"It read 172 lbs a few seconds ago\", you complain to your co-worker. \n",
    "\n",
    "\"I never said it was accurate,\" she replies.\n",
    "\n",
    "Sensors are inaccurate. This is the motivation behind a huge body of work in filtering, and solving this problem is the topic of this book. I could just provide the solutions that have been developed over the last half century, but these solutions were developed by asking very basic, fundamental questions into the nature of what we know and how we know it. Before we attempt the math, let's follow that journey of discovery, and see if it informs our intuition about filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try Another Scale**\n",
    "\n",
    "Is there any way we can improve upon this result? The obvious, first thing to try is get a better sensor. Unfortunately, your co-worker informs you that she has built 10 scales, and they all operate with about the same accuracy. You have her bring out another scale, and you weigh yourself on one, and then on the other. The first scale (A) reads \"160 lbs\", and the second (B) reads \"170 lbs\". What can we conclude about your weight?\n",
    "\n",
    "Well, what are our choices?\n",
    "\n",
    "* We could choose to only believe A, and assign 160lbs to our weight estimate.\n",
    "* We could choose to only believe B, and assign 170lbs to our weight.\n",
    "* We could choose a number less than both A and B.\n",
    "* We could choose a number greater than both A and B.\n",
    "* We could choose a number between A and B.\n",
    "\n",
    "The first two choices are plausible, but we have no reason to favor one scale over the other. Why would we choose to believe A instead of B? We have no reason for such a belief. The third and fourth choices are irrational. The scales are admittedly not very accurate, but there is no reason at all to choose a number outside of the range of what they both measured. The final choice is the only reasonable one. If both scales are inaccurate, and as likely to give a result above my actual weight as below it, more often than not the answer is somewhere between A and B. \n",
    "\n",
    "In mathematics this concept is formalized as [*expected value*](https://en.wikipedia.org/wiki/Expected_value), and we will cover it in depth later. For now ask yourself what would be the 'usual' thing to happen if we took one million readings. Some of the times both scales will read too low, sometimes both will read too high, and the rest of the time they will straddle the actual weight. If they straddle the actual weight then certainly we should choose a number between A and B. If they don't straddle then we don't know if they are both too high or low, but by choosing a number between A and B we at least mitigate the effect of the worst measurement. For example, suppose our actual weight is 180 lbs. 160 lbs is a big error. But if we choose a weight between 160 lbs and 170 lbs our estimate will be better than 160 lbs. The same argument holds if both scales returned a value greater than the actual weight.\n",
    "\n",
    "We will deal with this more formally later, but for now I hope it is clear that our best estimate is the average of A and B. \n",
    "\n",
    "$$\\frac{160+170}{2} = 165$$\n",
    "\n",
    "We can look at this graphically. I have plotted the measurements of A and B with an assumed error of $\\pm$ 8 lbs. The measurements falls between 160 and 170 so the only weight that makes sense must lie within 160 and 170 lbs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kf_book.book_plots as book_plots\n",
    "from kf_book.book_plots import plot_errorbars\n",
    "plot_errorbars([(160, 2, 'A'), (170, 4, 'B')], xlims=(150, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errorbars?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word on how I generated this plot. I import code from the module book_plots in the `kf_book` subdirectory. Generating this plot takes a lot of boilerplate Python that isn't interesting to read. I take this tack often in the book. When the cell is run `plot_errorbars()` gets called and the plot is inserted into the book.\n",
    "\n",
    "If this is your first time using [Jupyter Notebook](http://jupyter.org/), the code above is in a *cell*. The text \"In [2]:\" labels this as a cell where you can enter input, and the number in the bracket denotes that this cell was run second. To run the cell, click on it with your mouse so that it has focus, then press CTRL+ENTER on the keyboard. As we continue you will be able to alter the code inside the cells and rerun them. Try changing the values \"160\", \"170\", and \"8\" to some other value and run the cell. The printed output should change depending on what you entered.\n",
    "\n",
    "If you want to view the code for plot_errorbars, either open it in an editor, or create a new cell and type the function name followed by two question marks. Press Ctrl+Enter, and your browser will open a window displaying the source code. This is a feature of Jupyter Notebooks. If you want to just view the documentation for the function, do the same but with one question mark.\n",
    "\n",
    "```Python\n",
    "\n",
    "    plot_errorbars??\n",
    "```\n",
    "or\n",
    "```Python\n",
    "    plot_errorbars?\n",
    "```\n",
    "\n",
    "So 165 lbs looks like a reasonable estimate, but there is more information here that we might be able to take advantage of. The only weights that are possible lie in the intersection between the error bars of A and B. For example, a weight of 161 lbs is impossible because scale B could not give a reading of 170 lbs with a maximum error of 8 pounds. Likewise a weight of 169 lbs is impossible because scale A could not give a reading of 160 lbs with a maximum error of 8 lbs. In this example the only possible weights lie in the range of 162 to 168 lbs.\n",
    "\n",
    "That doesn't yet allow us to find a better weight estimate, but let's play 'what if' some more. What if we are now told that A is three times more accurate than B? Consider the 5 options we listed above. It still makes no sense to choose a number outside the range of A and B, so we will not consider those. It perhaps seems more compelling to choose A as our estimate - after all, we know it is more accurate, why not use it instead of B? Can B possibly improve our knowledge over A alone?\n",
    "\n",
    "The answer, perhaps counter intuitively, is yes, it can. First, let's look at the same measurements of A=160 and B=170, but with the error of A $\\pm$ 3 lbs and the error of B is 3 times as much, $\\pm$ 9 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errorbars([(160, 3, 'A'), (170, 9, 'B')], xlims=(150, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overlap of the error bars of A and B are the only possible true weight. This overlap is smaller than the error in A alone. More importantly, in this case we can see that the overlap doesn't include 160 lbs or 165 lbs. If we only used the measurement from A because it is more accurate than B we would give an estimate of 160 lbs. If we average A and B we would get 165 lbs. Neither of those weights are possible given our knowledge of the accuracy of the scales. By including the measurement of B we would give an estimate somewhere between 161 lbs and 163 lbs, the limits of the intersections of the two error bars.\n",
    "\n",
    "Let's take this to the extreme limits.  Assume we know scale A is accurate to 1 lb. In other words, if we truly weigh 170 lbs, it could report 169, 170, or 171 lbs. We also know that scale B is accurate to 9 lbs. We do a weighing on each scale, and get A=160, and B=170. What should we estimate our weight to be? Let's look at that graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errorbars([(160, 1, 'A'), (170, 9, 'B')], xlims=(150, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the only possible weight is 161 lbs. This is an important result. With two relatively inaccurate sensors we are able to deduce an extremely accurate result.\n",
    "\n",
    "**So two sensors, even if one is less accurate than the other, is better than one.** I will harp on this for the remainder of the book. We never throw information away, no matter how poor it is. We will be developing math and algorithms that allow us to include all possible sources of information to form the best estimate possible.\n",
    "\n",
    "However, we have strayed from our problem. No customer is going to want to buy multiple scales, and besides, we initially started with an assumption that all scales were equally (in)accurate. This insight of using all measurements regardless of accuracy will play a large role later, so don't forget it.\n",
    "\n",
    "What if I have one scale, but I weigh myself many times? We concluded that if we had two scales of equal accuracy we should average the results of their measurements. What if I weigh myself 10,000 times with one scale? We have already stated that the scale is equally likely to return a number too large as it is to return one that is too small. It is not that hard to prove that the average of a large number of weights will be very close to the actual weight, but let's write a simulation for now. I will use NumPy, part of the [SciPy](https://scipy.org/) ecosystem for numerical computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "measurements = np.random.uniform(160, 170, size=10000)\n",
    "mean = measurements.mean()\n",
    "print(f'Average of measurements is {mean:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact number printed depends on your random number generator, but it should be very close to 165.\n",
    "\n",
    "This code makes one assumption that probably isn't true - that the scale is as likely to read 160 as 165 for a true weight of 165 lbs. This is almost never true. Real sensors are more likely to get readings nearer the true value, and are less and less likely to get readings the further away from the true value it gets. We will cover this in detail in the Gaussian chapter. For now, I will use without further explanation the `numpy.random.normal()` function, which will produce more values nearer 165 lbs, and fewer further away. Take it on faith for now that this will produce noisy measurements similar to how a real scale works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.random.normal(165, 5, size=10000).mean()\n",
    "print(f'Average of measurements is {mean:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the answer is very close to 165. \n",
    "\n",
    "Okay, great, we have an answer to our sensor problem! But it is not a very practical answer. No one has the patience to weigh themselves ten thousand, or even a dozen times. \n",
    "\n",
    "So, let's play 'what if'. What if you measured your weight once a day, and got the readings 170, 161, and then 169. Did you gain weight, lose weight, or is this all just noisy measurements? \n",
    "\n",
    "We really can't say. The first measurement was 170, and the last was 169, implying a 1 lb loss. But if the scale is only accurate to 10 lbs, that is explainable by noise. I could have actually gained weight; maybe my weight on day one was 165 lbs, and on day three it was 172. It is possible to get those weight readings with that weight gain. My scale tells me I am losing weight, and I am actually gaining weight! Let's look at that in a chart. I've plotted the measurements along with the error bars, and then some possible weight gain/losses that could be explained by those measurements in dotted green lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kf_book.gh_internal as gh\n",
    "gh.plot_hypothesis1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is an extreme range of weight changes that could be explained by these three measurements. In fact, there are an infinite number of choices. Shall we give up? Not me! Recall that we are talking about measuring a human's weight. There is no reasonable way for a human to weigh 180 lbs on day 1 and 160 lbs on day 3. or to lose 30 lbs in one day only to gain it back the next (we will assume no amputations or other trauma has happened to the person). \n",
    "\n",
    "The behavior of the physical system we are measuring should influence how we interpret the measurements. If we were weighing a rock each day we'd attribute all of the variance to noise. If we were weighing a cistern fed by rain and used for household chores we might believe such weight changes are real.\n",
    " \n",
    "Suppose I take a different scale, and I get the following measurements: 169, 170, 169, 171, 170, 171, 169, 170, 169, 170. What does your intuition tell you? It is possible, for example, that you gained 1 lb each day, and the noisy measurements just happens to look like you stayed the same weight. Equally, you could have lost 1 lb a day and gotten the same readings. But is that likely? How likely is it to flip a coin and get 10 heads in a row? Not very likely. We can't prove it based solely on these readings, but it seems pretty likely that my weight held steady. In the chart below I've plotted the measurements with error bars, and a likely true weight in dashed green. This dashed line is not meant to be the 'correct' answer to this problem, merely one that is reasonable and could be explained by the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.plot_hypothesis2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another what if: what if the readings were 158.0, 164.2, 160.3, 159.9, 162.1, 164.6, 169.6, 167.4, 166.4, 171.0? Let's look at a chart of that and then answer some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.plot_hypothesis3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it 'seem' likely that I lost weight and this is just really noisy data? Not really. Does it seem likely that I held the same weight? Again, no. This data trends upwards over time; not evenly, but definitely upwards. We can't be sure, but that looks like a weight gain, and a significant weight gain at that. Let's test this assumption with some more plots. It is often easier to 'eyeball' data in a chart versus a table.\n",
    "\n",
    "So let's look at two hypotheses. First, let's assume our weight did not change. To get that number we agreed that we should average the measurements. Let's look at that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gh.plot_hypothesis4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look very convincing. In fact, we can see that there is no horizontal line that we could draw that is inside all of the error bars.\n",
    "\n",
    "Now, let's assume we gained weight. How much? I don't know, but NumPy does! We want to draw a line through the measurements that looks 'about' right. NumPy has functions that will do this according to a rule called \"least squares fit\". Let's not worry about the details of that computation (I use [polyfit()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) if you are interested), and just plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.plot_hypothesis5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better, at least to my eyes. Notice now the hypothesis lies very close to each measurement, whereas in the previous plot the hypothesis was often quite far from the measurement. It seems far more likely to be true that I gained weight than I didn't gain any weight. Did I actually gain 13 lbs? Who can say? That seems impossible to answer.\n",
    "\n",
    "\"But is it impossible?\" pipes up a co-worker.\n",
    "\n",
    "Let's try something crazy. Let's assume that I know I am gaining about one lb a day. It doesn't matter how I know that right now, assume I know it is approximately correct. Maybe I am on a 6000 calorie a day diet, which would result in such a weight gain. Or maybe there is another way to estimate the weight gain. This is a thought experiment, the details are not important. Let's see if we can make use of such information if it was available.\n",
    "\n",
    "The first measurement was 158. We have no way of knowing any different, so let's accept that as our estimate. If our weight today is 158, what will it be tomorrow? Well, we think we are gaining weight at 1 lb/day, so our prediction is 159, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.plot_estimate_chart_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, but what good is this? Sure, we could assume the 1 lb/day is accurate, and predict our weight for the next 10 days, but then why use a scale at all if we don't incorporate its readings? So let's look at the next measurement. We step on the scale again and it displays 164.2 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh.plot_estimate_chart_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a problem. Our prediction doesn't match our measurement. But, that is what we expected, right? If the prediction was always exactly the same as the measurement, it would not be capable of adding any information to the filter. And, of course, there would be no reason to ever measure since our predictions are perfect.\n",
    "\n",
    "> **The key insight to this entire book is in the next paragraph. Read it carefully!**\n",
    "\n",
    "So what do we do? If we only form estimates from the measurement then the prediction will not affect the result. If we only form estimates from the prediction then the measurement will be ignored. If this is to work we need to take some kind of **blend of the prediction and measurement** (I've bolded the key point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blending two values - this sounds a lot like the two scale problem earlier. Using the same reasoning as before we can see that the only thing that makes sense is to choose a number between the prediction and the measurement. For example, an estimate of 165 makes no sense, nor does 157. Our estimates should lie between 159 (the prediction) and 164.2 (the measurement).\n",
    "\n",
    "One more time, this is so important. We agreed that when presented two values with errors, we should form an estimate part way between the two values. It does not matter how those values were generated. In the start of the chapter we had two measurements, but now we have one measurement and one prediction. The reasoning, and hence the math is the same in both cases. We *never* throw information away. I mean it. I see so much commercial software that throws away noisy data. Don't do it! Our prediction of a weight gain might not be very accurate, but so long as there is some information we should use it.\n",
    "\n",
    "I have to insist you stop and really think about this. All I have done is replaced an inaccurate scale with an inaccurate weight prediction based on human physiology. It is still data. Math doesn't know if the data came from a scale or a prediction. We have two pieces of data with a certain amount of noise, and we want to combine them. In the remainder of this book we are going to develop some fairly complicated math to perform this computation, but the math never cares where the data come from, it only makes computations based on the value and accuracy of those values. \n",
    "\n",
    "Should the estimate be half way between the measurement and prediction? Maybe, but in general it seems like we might know that our prediction is more or less accurate compared to the measurements. Probably the accuracy of our prediction differs from the accuracy of the scale. Recall what we did when scale A was much more accurate than scale B - we scaled the answer to be closer to A than B. Let's look at that in a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gh.plot_estimate_chart_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a randomly chosen number to scale our estimate: $\\frac{4}{10}$. Our estimate will be four tenths the measurement and the rest will be from the prediction. In other words, we are expressing a belief here, a belief that the prediction is somewhat more likely to be correct than the measurement. We compute that as\n",
    "\n",
    "$$\\mathtt{estimate} = \\mathtt{prediction} + \\frac{4}{10}(\\mathtt{measurement} - \\mathtt{prediction})$$\n",
    "\n",
    "The difference between the measurement and prediction is called the *residual*, which is depicted by the black vertical line in the plot above. This will become an important value to use later on, as it is an exact computation of the difference between measurements and the filter's output. Smaller residuals imply better performance.\n",
    "\n",
    "Let's code that and see the results when we test it against the series of weights from above. We have to take into account one other factor. Weight gain has units of lbs/time, so to be general we will need to add a time step $t$, which we will set to 1 (day). \n",
    "\n",
    "I hand generated the weight data to correspond to a true starting weight of 160 lbs, and a weight gain of 1 lb per day. In other words on the first day (day zero) the true weight is 160lbs, on the second day (day one, the first day of weighing) the true weight is 161 lbs, and so on. \n",
    "\n",
    "We need to make a guess for the initial weight. It is too early to talk about initialization strategies, so for now I will assume 160 lbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kf_book.book_plots import figsize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights = [158.0, 164.2, 160.3, 159.9, 162.1, 164.6, \n",
    "           169.6, 167.4, 166.4, 171.0, 171.2, 172.6]\n",
    "\n",
    "time_step = 1.0  # day\n",
    "scale_factor = 4.0/10\n",
    "\n",
    "def predict_using_gain_guess(estimated_weight, gain_rate, do_print=False):     \n",
    "    # storage for the filtered results\n",
    "    estimates, predictions = [estimated_weight], []\n",
    "\n",
    "    # most filter literature uses 'z' for measurements\n",
    "    for z in weights: \n",
    "        # predict new position\n",
    "        predicted_weight = estimated_weight + gain_rate * time_step\n",
    "\n",
    "        # update filter \n",
    "        estimated_weight = predicted_weight + scale_factor * (z - predicted_weight)\n",
    "\n",
    "        # save and log\n",
    "        estimates.append(estimated_weight)\n",
    "        predictions.append(predicted_weight)\n",
    "        if do_print:\n",
    "            gh.print_results(estimates, predicted_weight, estimated_weight)\n",
    "\n",
    "    return estimates, predictions\n",
    "\n",
    "initial_estimate = 160.\n",
    "estimates, predictions = predict_using_gain_guess(\n",
    "    estimated_weight=initial_estimate, gain_rate=1, do_print=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "book_plots.set_figsize(10)\n",
    "gh.plot_gh_results(weights, estimates, predictions, [160, 172])\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty good! There is a lot of data here, so let's talk about how to interpret it.  The thick blue line shows the estimate from the filter. It starts at day 0 with the initial guess of 160 lbs. The red line shows the prediction that is made from the previous day's weight. So, on day one the previous weight was 160 lbs, the weight gain is 1 lb, and so the first prediction is 161 lbs. The estimate on day one is then part way between the prediction and measurement at 159.8 lbs. Below the chart is a print out of the previous weight, predicted weight, and new estimate for each day. Finally, the thin black line shows the actual weight gain of the person being weighed.\n",
    "\n",
    "Walk through this for each day, ensuring you understand how the prediction and estimates were formed at each step. Note how the estimate always falls between the measurement and prediction.\n",
    "\n",
    "The estimates are not a straight line, but they are straighter than the measurements and somewhat close to the trend line we created. Also, it seems to get better over time.\n",
    "\n",
    "The results of the filter may strike you as quite silly; of course the data will look good if we assume the conclusion, that our weight gain is around 1 lb/day! Let's see what the filter does if our initial guess is bad. Let's predict that there is a weight loss of 1 lb a day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, p = predict_using_gain_guess(initial_estimate, -1.)\n",
    "gh.plot_gh_results(weights, e, p, [160, 172])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not so impressive. The estimates quickly divert from the measurements. Clearly a filter that requires us to correctly guess a rate of change is not very useful. Even if our initial guess was correct, the filter will fail as soon as that rate of change changes. If I stop overeating the filter will have extreme difficulty in adjusting to that change. Note that it is adjusting! The estimates are climbing even though we tell it we are losing 1 lb a day. It just can't adjust fast enough.\n",
    "\n",
    "But, 'what if'? What if instead of leaving the weight gain at the initial guess of 1 lb (or whatever), we compute it from the existing measurements and estimates. On day one our estimate for the weight is:\n",
    "\n",
    "$$\n",
    "(160 + 1) + \\frac{4}{10}(158-161) = 159.8\n",
    "$$\n",
    "\n",
    "On the next day we measure 164.2, which implies a weight gain of 4.4 lbs (since 164.2 - 159.8 = 4.4), not 1. Can we use this information somehow? It seems plausible. After all, the weight measurement itself is based on a real world measurement of our weight, so there is useful information. Our estimate of our weight gain may not be perfect, but it is surely better than just guessing our gain is 1 lb. Data is better than a guess, even if it is noisy.\n",
    "\n",
    "People really balk at this point, so make sure you are in agreement. Two noisy measurements of weight give us an implied weight gain/loss. That estimate is going to be very inaccurate if the measurements are inaccurate, but there is still information in this computation. Imagine weighing a cow with a scale accurate to 1 lb, and it shows that the cow gained 10 lbs. The cow might have gained 8 lbs up to 12 lbs, depending on the errors, but we know it gained weight, and roughly how much. This is information. What do we do with information? Never throw it away!\n",
    "\n",
    "Back to my diet. Should we set the new gain/day to 4.4 lbs? Yesterday we thought the weight gain was 1 lb, today we think it is 4.4 lbs. We have two numbers, and want to combine them somehow. Hmm, sounds like our same problem again. Let's use our same tool, and the only tool we have so far - pick a value part way between the two. This time I will use another arbitrarily chosen number, $\\frac{1}{3}$. The equation is identical as for the weight estimate except we have to incorporate time because this is a rate (gain/day):\n",
    "\n",
    "$$\\text{new gain} = \\text{old gain} + \\frac{1}{3}\\frac{\\text{measurement - predicted weight}}{1 \\text{ day}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight    = 160.  # initial guess\n",
    "gain_rate = -1.0  # initial guess\n",
    "\n",
    "time_step    = 1\n",
    "weight_scale = 4./10\n",
    "gain_scale   = 1./3\n",
    "estimates    = [weight]\n",
    "predictions  = []\n",
    "\n",
    "for z in weights:           # weights are measurement\n",
    "    # prediction step\n",
    "    weight    = weight + gain_rate*time_step    # prediction\n",
    "    gain_rate = gain_rate\n",
    "    predictions.append(weight)\n",
    "    \n",
    "    # update step    \n",
    "    residual = z - weight       # measurement - prediction\n",
    "    \n",
    "    gain_rate = gain_rate + gain_scale   * (residual/time_step)     # update gain\n",
    "    weight    = weight    + weight_scale * residual                 # estimation\n",
    "  \n",
    "    print(gain_rate)\n",
    "    estimates.append(weight)\n",
    "gh.plot_gh_results(weights, estimates, predictions, [160, 172])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is starting to look really good. Because of the poor initial guess of the weight gain being -1 it takes the filter several days to accurately predict the weight, but once it does that it starts to accurately track the weight. We used no methodology for choosing our scaling factors of $\\frac{4}{10}$ and $\\frac{1}{3}$ (actually, they are poor choices for this problem), but otherwise all of the math followed from very reasonable assumptions. Recall that you can change the value of the parameter `time_step` to a larger value and re-run the cell if you want to see the plot drawn step-by-step.\n",
    "\n",
    "One final point before we go on. In the prediction step I wrote the line\n",
    "```python\n",
    "gain_rate = gain_rate\n",
    "``` \n",
    "This obviously has no effect, and can be removed. I wrote this to emphasize that in the prediction step you need to predict the next value for all variables, both `weight` and `gain_rate`. This will become relevant shortly. In this case we are assuming that the gain does not vary, but when we generalize this algorithm we will remove that assumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The g-h Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is known as the [g-h filter](https://en.wikipedia.org/wiki/Alpha_beta_filter) or the $\\alpha$-$\\beta$ filter. $g$ and $h$ refer to the two scaling factors that we used in our example. $g$ is the scaling we used for the measurement (weight in our example), and $h$ is the scaling for the change in measurement over time (lbs/day in our example). $\\alpha$ and $\\beta$ are just different names used for this factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This filter is the basis for a huge number of filters, including the Kalman filter. In other words, the Kalman filter is a form of the g-h filter, which I will prove later in the book. So is the Least Squares filter, which you may have heard of, and so is the Benedict-Bordner filter, which you probably have not. Each filter has a different way of assigning values to $g$ and $h$, but otherwise the algorithms are identical. For example, the Benedict-Bordner filter assigns a constant to $g$ and $h$, constrained to a certain range of values. Other filters such as the Kalman will vary $g$ and $h$ dynamically at each time step.\n",
    "\n",
    "**Let me repeat the key points as they are so important**. If you do not understand these you will not understand the rest of the book. If you do understand them, then the rest of the book will unfold naturally for you as mathematical elaborations to various 'what if' questions we will ask about $g$ and $h$. The math may look profoundly different, but the algorithm will be exactly the same.\n",
    "\n",
    "* Multiple data points are more accurate than one data point, so throw nothing away no matter how inaccurate it is.\n",
    "* Always choose a number part way between two data points to create a more accurate estimate.\n",
    "* Predict the next measurement and rate of change based on the current estimate and how much we think it will change.\n",
    "* The new estimate is then chosen as part way between the prediction and next measurement scaled by how accurate each is.\n",
    "\n",
    "Let's look at a visual depiction of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_plots.predict_update_chart()\n",
    "\n",
    "# note:\n",
    "# 图中似乎有错误。State Estimat 是在 Update Stap完成的，而不是Predict Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me introduce some more formal terminology. The *system* is the object that we want to estimate. In this chapter the system is whatever we are trying to weigh. Some texts call this the *plant*. That terminology comes from control system theory. https://en.wikipedia.org/wiki/Plant_(control_theory)\n",
    "\n",
    "The *state* of the system is the current configuration or values of that system that is of interest to us. We are interested only in the weight reading. If I put a 100 kg weight on the scale, the state is 100kg. We define the state based on what is relevant to us. The color of the scale is irrelevant to us so we do not include those values in the state. A QA engineer for the manufacturer might include color in the state so that she can track and control the manufacturing process. \n",
    "\n",
    "The *measurement* is a measured value of the system. Measurements can be inaccurate, so it may not have the same value as the state.\n",
    "\n",
    "The *state estimate* is our filter's estimate of the state. For example, for the 100 kg weight our estimate might be 99.327 kg due to sensor errors. This is commonly abbreviated to *estimate*, and I have done that in this chapter.\n",
    "\n",
    "In other words, the state should be understood as the actual value of the system. This value is usually *hidden* to us.  If I stepped on a scale you'd then have a *measurement*. We call this *observable* since you can directly observe this measurement. In contrast, you can never directly observe my weight, you can only measure it. \n",
    "\n",
    "This language of *hidden* and *observable* is important. Any estimation problem consists of forming an estimate of a hidden state via observable measurements. If you read the literature these terms are used when defining a problem, so you need to be comfortable with them.\n",
    "\n",
    "We use a *process model* to mathematically model the system. In this chapter our process model is the assumption that my weight today is yesterday's weight plus my weight gain for the last day. The process model does not model or otherwise account for the sensors. Another example would be a process model for an automobile. The process model might be \"distance equals velocity times time. This model is not perfect as the velocity of a car can vary over a non-zero amount of time, the tires can slip on the road, and so on. The *system error* or *process error* is the error in this model. We never know this value exactly; if we did we could refine our model to have zero error. Some texts use *plant model* and *plant error*. You may also see *system model*. They all mean the same thing.\n",
    "\n",
    "The predict step is known as *system propagation*. It uses the *process model* to form a new *state estimate*. Because of the *process error* this estimate is imperfect. Assuming we are tracking data over time, we say we *propagate* the state into the future. Some texts call this the *evolution*. \n",
    "\n",
    "The update step is known as the *measurement update*. One iteration of the system propagation and measurement update is known as an *epoch*. \n",
    "\n",
    "Now let's explore a few different problem domains to better understand this algorithm. Consider the problem of trying to track a train on a track. The track constrains the position of the train to a very specific region. Furthermore, trains are large and slow. It takes many minutes for them to slow down or speed up significantly. So, if I know that the train is at kilometer marker 23 km at time t and moving at 18 kph, I can be extremely confident in predicting its position at time t + 1 second. Why is that important? Suppose we can only measure its position with an accuracy of $\\pm$ 250 meters. The train is moving at 18 kph, which is 5 meters per second. At t+1 seconds the train will be at 23.005 km yet the measurement could be anywhere from 22.755 km to 23.255 km. So if the next measurement says the position is at 23.4 we know that must be inaccurate. Even if at time t the engineer slammed on the brakes the train will still be very near to 23.005 km because a train cannot slow down very much in 1 second. If we were to design a filter for this problem (and we will a bit further in the chapter!) we would want to design a filter that gave a very high weighting to the prediction vs the measurement. \n",
    "\n",
    "Now consider the problem of tracking a thrown ball. We know that a ballistic object moves in a parabola in a vacuum when in a gravitational field. But a ball thrown on Earth is influenced by air drag, so it does not travel in a perfect parabola. Baseball pitchers take advantage of this fact when they throw curve balls. Let's say that we are tracking the ball inside a stadium using computer vision, something I do at work. The accuracy of the computer vision tracking might be modest, but predicting the ball's future positions by assuming that it is moving on a parabola is not extremely accurate either. In this case we'd probably design a filter that gave roughly equal weight to the measurement and the prediction.\n",
    "\n",
    "Now consider trying to track a helium party balloon in a hurricane. We have no legitimate model that would allow us to predict the balloon's behavior except over very brief time scales (we know the balloon cannot go 10 miles in 1 second, for example). In this case we would design a filter that emphasized the measurements over the predictions.\n",
    "\n",
    "Most of this book is devoted to expressing the concerns in the last three paragraphs mathematically, which then allows us to find an optimal solution (in some mathematical sense). In this chapter we will merely be assigning different values to $g$ and $h$ in a more intuitive, and thus less optimal way. But the fundamental idea is to blend somewhat inaccurate measurements with somewhat inaccurate models of how the systems behaves to get a filtered estimate that is better than either information source by itself.\n",
    "\n",
    "We can express this as an algorithm:\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "    1. Initialize the state of the filter\n",
    "    2. Initialize our belief in the state\n",
    "\n",
    "**Predict**\n",
    "\n",
    "    1. Use system behavior to predict state at the next time step\n",
    "    2. Adjust belief to account for the uncertainty in prediction\n",
    "    \n",
    "**Update**\n",
    "\n",
    "    1. Get a measurement and associated belief about its accuracy\n",
    "    2. Compute residual between estimated state and measurement\n",
    "    3. New estimate is somewhere on the residual line\n",
    "    \n",
    "We will use this same algorithm throughout the book, albeit with some modifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "I'll begin to introduce the notations and variable names used in the literature. Some of this was already used in the above charts. Measurement is typically denoted $z$ and that is what we will use in this book (some literature uses $y$). Subscript $k$ indicates the time step, so $z_k$ is the data for this time step. A bold font denotes a vector or matrix. So far we have only considered having one sensor, and hence one sensor measurement, but in general we may have *n* sensors and *n* measurements. $\\mathbf{x}$ denotes our state, and is bold to denote that it is a vector. For our scale example, it represents both the initial weight and initial weight gain rate, like so:\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix}x \\\\ \\dot{x}\\end{bmatrix}$$\n",
    "\n",
    "Here I use Newton's notation of a dot over the x to denote velocity. More precisely, the dot implies the derivative of x with respect to time, which of course is the velocity. For a weight of 62 kg with a gain of 0.3 kg/day we have\n",
    "\n",
    "$$\\mathbf{x} = \\begin{bmatrix}62 \\\\ 0.3\\end{bmatrix}$$\n",
    "\n",
    "So, the algorithm is simple. The state is initialized with $\\mathbf{x_0}$, the initial estimate. We then enter a loop, predicting the state for time or step $k$ from the values from time (or step) $k-1$. We then get the measurement $z_k$ and choose some intermediate point between the measurements and prediction, creating the estimate $\\mathbf{x}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Write Generic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, I explicitly coded this to solve the weighing problem that we've been discussing throughout the chapter. For example, the variables are named \"weight_scale\", \"gain\", and so on. I did this to make the algorithm easy to follow - you can easily see that we correctly implemented each step. But, that is code written for exactly one problem, and the algorithm is the same for any problem. So let's rewrite the code to be generic - to work with any problem. Use this function signature:\n",
    "\n",
    "Note:\n",
    "For Radar, $g$ is the gain for position, and $h$ is for velocity.\n",
    "**Given larger value, the impact of current measurement will be much greater.**\n",
    "\n",
    "```python\n",
    "def g_h_filter(data, x0, dx, g, h, dt):\n",
    "    \"\"\"\n",
    "    Performs g-h filter on 1 state variable with a fixed g and h.\n",
    "\n",
    "    'data' contains the data to be filtered.                (measurement, eg. position)\n",
    "    'x0' is the initial value for our state variable.       (init position)\n",
    "    'dx' is the initial change rate for our state variable  (veocity)\n",
    "    'g' is the g-h's g scale factor                         (gain for position, or for prediction)\n",
    "    'h' is the g-h's h scale factor                         (gain for velocity, or for prediction)\n",
    "    'dt' is the length of the time step                     (measurement interval)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Return the data as a NumPy array, not a list. Test it by passing in the same weight data as before, plot the results, and visually determine that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "from kf_book.gh_internal import plot_g_h_results\n",
    "def g_h_filter(data, x0, dx, g, h, dt):\n",
    "    # pass #  your solution here\n",
    "    x_est   = x0      # init\n",
    "    results = []\n",
    "\n",
    "    for z in data:\n",
    "        # prediction\n",
    "        x_predict = x_est + dx*dt\n",
    "        dx        = dx\n",
    "\n",
    "        # update\n",
    "        residual = z-x_predict\n",
    "        dx      = dx + h*residual/dt\n",
    "        x_est   = x_predict + g*residual/dt\n",
    "\n",
    "        # save\n",
    "        results.append(x_est)\n",
    "    return np.asarray(results)\n",
    "\n",
    "# uncomment to run the filter and plot the results\n",
    "book_plots.plot_track([0, 11], [160, 172], label='Actual weight')\n",
    "data = g_h_filter(data=weights, x0=160., dx=1., g=6./10, h=2./3, dt=1.)\n",
    "plot_g_h_results(weights, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "\n",
    "def g_h_filter(data, x0, dx, g, h, dt=1.):\n",
    "    x_est = x0\n",
    "    results = []\n",
    "    for z in data:\n",
    "        # prediction step\n",
    "        x_pred = x_est + (dx*dt)\n",
    "        dx     = dx\n",
    "\n",
    "        # update step\n",
    "        residual = z - x_pred\n",
    "        dx = dx + h * (residual) / dt\n",
    "        x_est = x_pred + g * residual\n",
    "        results.append(x_est)\n",
    "    return np.array(results)\n",
    "\n",
    "book_plots.plot_track([0, 11], [160, 172], label='Actual weight')\n",
    "data = g_h_filter(data=weights, x0=160., dx=1., g=6./10, h=2./3, dt=1.)\n",
    "plot_g_h_results(weights, data)\n",
    "print(weights)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should have been straightforward. I just subtituted the variable names `x0`, `dx`, etc., for the variable names in the weight gain code. Nothing else needed to change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of $g$ and $h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The g-h filter is not one filter - it is a classification for a family of filters. Eli Brookner in *Tracking and Kalman Filtering Made Easy* lists 11, and I am sure there are more. Not only that, but each type of filter has numerous subtypes. Each filter is differentiated by how $g$ and $h$ are chosen. So there is no 'one size fits all' advice that I can give here. Some filters set $g$ and $h$ as constants, others vary them dynamically. The Kalman filter varies them dynamically at each step. Some filters allow $g$ and $h$ to take any value within a range, others constrain one to be dependent on the other by some function $f(\\dot{})$, where $g = f(h)$.\n",
    "\n",
    "The topic of this book is not the entire family of g-h filters; more importantly, we are interested in the *Bayesian* aspect of these filters, which I have not addressed yet. Therefore I will not cover selection of $g$ and $h$ in depth. *Tracking and Kalman Filtering Made Easy* is an excellent resource for that topic. If this strikes you as an odd position for me to take, recognize that the typical formulation of the Kalman filter does not use $g$ and $h$ at all. The Kalman filter is a g-h filter because it mathematically reduces to this algorithm. When we design the Kalman filter we use design criteria that can be mathematically reduced to $g$ and $h$, but the Kalman filter form is usually a much more powerful way to think about the problem. Don't worry if this is not too clear right now, it will clear once we develop the Kalman filter theory.\n",
    "\n",
    "It is worth seeing how varying $g$ and $h$ affects the results, so we will work through some examples. This will give us strong insight into the fundamental strengths and limitations of this type of filter, and help us understand the behavior of the rather more sophisticated Kalman filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: create measurement function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that generates noisy data for us. In this book I model a noisy signal as the signal plus [white noise](https://en.wikipedia.org/wiki/White_noise). We've not yet covered the statistics to fully understand the definition of white noise. In essence, think of it as data that randomly varies higher and lower than the signal with no pattern. We say that it is a serially uncorrelated random variable with zero mean and finite variance. If you don't follow that, you will by the end of the *Gaussians* chapter. You may not be successful at this exercise if you have no knowledge of statistics. If so, just read the solution and discussion.\n",
    "\n",
    "White noise can be generated by `numpy.random.randn()`. We want a function that we call with the starting value, the amount of change per step, the number of steps, and the amount of noise we want to add. It should return a list of the data. Test it by creating 30 points, filtering it with `g_h_filter()`, and plot the results with `plot_g_h_results()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "def gen_data(x0, dx, count, noise_factor):\n",
    "    return [x0 + dx*i + randn()*noise_factor for i in range(count)]\n",
    "\n",
    "measurements = gen_data(0, 1, 30, 1)\n",
    "data = g_h_filter(data=measurements, x0=0., dx=1., dt=1., g=.2, h=0.02)\n",
    "plot_g_h_results(measurements, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "`randn()` returns random numbers centered around 0 - it is just as likely to be greater than zero as under zero. It varies by *one standard deviation* - don't worry if you don't know what that means. I've plotted 3000 calls to `randn()` - you can see that the values are centered around zero and mostly range from a bit under -1 to a bit more than +1, though occasionally they are much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([randn() for _ in range(3000)], lw=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bad Initial Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write code that uses `gen_data` and `g_h_filter` to filter 100 data points that starts at 5, has a derivative of 2, a noise scaling factor of 10, and uses g=0.2 and h=0.02. Set your initial guess for x to be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "x_measure = gen_data(x0=5,dx=2,count=100,noise_factor=10)\n",
    "x_est   = g_h_filter(data=x_measure,x0=100,dx=2,dt=1,g=0.2,h=0.02)\n",
    "plot_g_h_results(x_measure, x_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_data(x0=5., dx=2., count=100, noise_factor=10)\n",
    "data = g_h_filter(data=zs, x0=100., dx=2., dt=1., g=0.2, h=0.02)\n",
    "plot_g_h_results(measurements=zs, filtered_data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter starts out with estimates that are far from the measured data due to the bad initial guess of 100. You can see that it 'rings' before settling in on the measured data. 'Ringing' means that the signal overshoots and undershoots the data in a sinusoidal type pattern. This is a very common phenomena in filters, and a lot of work in filter design is devoted to minimizing ringing. That is a topic that we are not yet prepared to address, but I wanted to show you the phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Extreme Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the same test, but this time use a noise factor of 100. Remove the initial condition ringing by changing the initial condition from 100 down to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "x_measure = gen_data(x0=5,dx=2,count=100,noise_factor=100)\n",
    "x_est   = g_h_filter(data=x_measure,x0=5,dx=2,dt=1,g=0.01,h=0.01)\n",
    "plot_g_h_results(x_measure, x_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_data(x0=5., dx=2., count=100, noise_factor=100)\n",
    "data = g_h_filter(data=zs, x0=5., dx=2., dt = 1, g=0.2, h=0.02)\n",
    "plot_g_h_results(measurements=zs, filtered_data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't look so wonderful to me. We can see that perhaps the filtered signal varies less than the noisy signal, but it is far from the straight line. If we were to plot just the filtered result no one would guess that the signal starts at 5 and increments by 2 at each time step. And while in locations the filter does seem to reduce the noise, in other places it seems to overshoot and undershoot.\n",
    "\n",
    "At this point we don't know enough to really judge this. We added **a lot** of noise; maybe this is as good as filtering can get. However, the existence of the multitude of chapters beyond this one should suggest that we can do much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: The Effect of Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a new data generation function that adds in a constant acceleration factor to each data point. In other words, increment dx as you compute each data point so that the velocity (dx) is ever increasing. Set the noise to 0, $g=0.2$ and $h=0.02$ and plot the results using `plot_g_h_results` or your own routine. Play around with different accererations and times steps. Explain what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def gen_data(x0, dx, count, noise_factor, acc=0.):\n",
    "    return [x0 + dx*i + 0.5*acc*i**2 + randn()*noise_factor for i in range(count)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(x0, dx, count, noise_factor, accel=0.):\n",
    "    zs = []\n",
    "    for i in range(count):\n",
    "        zs.append(x0 + accel * (i**2) / 2 + dx*i + randn()*noise_factor)\n",
    "        dx += accel     # error?\n",
    "    return zs\n",
    "   \n",
    "predictions = []\n",
    "zs = gen_data(x0=10., dx=0., count=20, noise_factor=0, accel=9.)\n",
    "data = g_h_filter(data=zs, x0=10., dx=0., g=0.2, h=0.02)\n",
    "plot_g_h_results(measurements=zs, filtered_data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each prediction lags behind the signal. If you think about what is happening this makes sense. Our model assumes that velocity is constant. The g-h filter computes the first derivative of $x$ (we use $\\dot{x}$ to denote the derivative) but not the second derivative $\\ddot{x}$. So we are assuming that $\\ddot{x}=0$. At each prediction step we predict the new value of x as $x + \\dot{x}*t$. But because of the acceleration the prediction must necessarily fall behind the actual value. We then try to compute a new value for $\\dot{x}$, but because of the $h$ factor we only partially adjust $\\dot{x}$ to the new velocity. On the next iteration we will again fall short.\n",
    "\n",
    "Note that there is no adjustment to $g$ or $h$ that we can make to correct this problem. This is called the *lag error* or *systemic error* of the system. It is a fundamental property of g-h filters. Perhaps your mind is already suggesting solutions or workarounds to this problem. As you might expect, a lot of research has been devoted to this problem, and we will be presenting various solutions to this problem in this book.\n",
    "> The 'take home' point is that the filter is only as good as the mathematical model used to express the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Varying $g$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the effect of varying $g$. Before you perform this exercise, recall that $g$ is the scale factor for choosing between the measurement and prediction. What do you think the effect of a large value of $g$ will be? A small value?\n",
    "\n",
    "Now, let the `noise_factor=50` and `dx=5`. Plot the results of $g = 0.1, 0.4, \\text{and }  0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "zs = gen_data(x0=5., dx=5., count=50, noise_factor=50)\n",
    "data1 = g_h_filter(data=zs, x0=0., dx=5., dt=1., g=0.1, h=0.01)\n",
    "data2 = g_h_filter(data=zs, x0=0., dx=5., dt=1., g=0.4, h=0.01)\n",
    "data3 = g_h_filter(data=zs, x0=0., dx=5., dt=1., g=0.8, h=0.01)\n",
    "\n",
    "with book_plots.figsize(y=4):\n",
    "    book_plots.plot_measurements(zs, color='k')\n",
    "    book_plots.plot_filter(data1, label='g=0.1', marker='s', c='C0')\n",
    "    book_plots.plot_filter(data2, label='g=0.4', marker='v', c='C1')\n",
    "    book_plots.plot_filter(data3, label='g=0.8', c='C2')\n",
    "    plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that as $g$ is larger we more closely follow the measurement instead of the prediction. When $g=0.8$ we follow the signal almost exactly, and reject almost none of the noise. One might naively conclude that $g$ should always be very small to maximize noise rejection. However, that means that we are mostly ignoring the measurements in favor of our prediction. What happens when the signal changes not due to noise, but an actual state change? Let's have a look. I will create data that has $\\dot{x}=1$ for 9 steps before changing to $\\dot{x}=0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "for i in range(50):\n",
    "    zs.append(14)\n",
    "\n",
    "data1 = g_h_filter(data=zs, x0=4., dx=1., dt=1., g=0.1, h=0.01)\n",
    "data2 = g_h_filter(data=zs, x0=4., dx=1., dt=1., g=0.5, h=0.01)\n",
    "data3 = g_h_filter(data=zs, x0=4., dx=1., dt=1., g=0.9, h=0.01)\n",
    "\n",
    "book_plots.plot_measurements(zs)\n",
    "book_plots.plot_filter(data1, label='g=0.1', marker='s', c='C0')\n",
    "book_plots.plot_filter(data2, label='g=0.5', marker='v', c='C1')\n",
    "book_plots.plot_filter(data3, label='g=0.9', c='C3')\n",
    "plt.legend(loc=4)\n",
    "plt.ylim([6, 20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the effects of ignoring the signal. We not only filter out noise, but legitimate changes in the signal as well. \n",
    "\n",
    "Maybe we need a 'Goldilocks' filter, where $g$ is not too large, not too small, but just right? Well, not exactly. As alluded to earlier, different filters choose $g$ and $h$ in different ways depending on the mathematical properties of the problem. For example, the Benedict-Bordner filter was invented to minimize the transient error in this example, where $\\dot{x}$ makes a step jump. We will not discuss this filter in this book, but here are two plots chosen with different allowable pairs of $g$ and $h$. This filter design minimizes transient errors for step jumps in $\\dot{x}$ at the cost of not being optimal for other types of changes in $\\dot{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = [5,6,7,8,9,9,9,9,9,10,11,12,13,14,\n",
    "      15,16,16,16,16,16,16,16,16,16,16,16]\n",
    "data1 = g_h_filter(data=zs, x0=4., dx=1., dt=1., g=.302, h=.054)\n",
    "data2 = g_h_filter(data=zs, x0=4., dx=1., dt=1., g=.546, h=.205)\n",
    "\n",
    "book_plots.plot_measurements(zs)\n",
    "book_plots.plot_filter(data2, label='g=0.546, h=0.205', marker='s', c='C0')\n",
    "book_plots.plot_filter(data1, label='g=0.302, h=0.054', marker='v', c='C1')\n",
    "plt.legend(loc=4)\n",
    "plt.ylim([6, 18]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying $h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's leave $g$ unchanged and investigate the effect of modifying $h$. We know that $h$ affects how much we favor the measurement of $\\dot{x}$ vs our prediction. But what does this *mean*? If our signal is changing a lot (quickly relative to the time step of our filter), then a large $h$ will cause us to react to those transient changes rapidly. A smaller $h$ will cause us to react more slowly.\n",
    "\n",
    "We will look at three examples. We have a noiseless measurement that slowly goes from 0 to 1 in 50 steps. Our first filter uses a nearly correct initial value for $\\dot{x}$ and a small $h$. You can see from the output that the filter output is very close to the signal. The second filter uses the very incorrect guess of $\\dot{x}=2$. Here we see the filter 'ringing' until it settles down and finds the signal. The third filter uses the same conditions but it now sets $h=0.5$. If you look at the amplitude of the ringing you can see that it is much smaller than in the second chart, but the frequency is greater. It also settles down a bit quicker than the second filter, though not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = np.linspace(0, 1, 50)\n",
    "\n",
    "data1 = g_h_filter(data=zs, x0=0, dx=0., dt=1., g=.2, h=0.05)\n",
    "data2 = g_h_filter(data=zs, x0=0, dx=2., dt=1., g=.2, h=0.05)\n",
    "data3 = g_h_filter(data=zs, x0=0, dx=2., dt=1., g=.2, h=0.5)\n",
    "\n",
    "book_plots.plot_measurements(zs)\n",
    "book_plots.plot_filter(data1, label='dx=0, h=0.05', c='C0')\n",
    "book_plots.plot_filter(data2, label='dx=2, h=0.05', marker='v', c='C1')\n",
    "book_plots.plot_filter(data3, label='dx=2, h=0.5',  marker='s', c='C2')\n",
    "plt.legend(loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Example\n",
    "\n",
    "For those of you running this in Jupyter Notebook I've written an interactive version of the filter so you can see the effect of changing $\\dot{x}$, $g$ and $h$ in real time. As you adjust the sliders for $\\dot{x}$, $g$ and $h$ the data will be refiltered and the results plotted for you.\n",
    "\n",
    "If you really want to test yourself, read the next paragraph and try to predict the results before you move the sliders. \n",
    "\n",
    "Some things to try include setting $g$  and $h$ to their minimum values. See how perfectly the filter tracks the data! This is only because we are perfectly predicting the weight gain. Adjust $\\dot{x}$ to larger or smaller than 5. The filter should diverge from the data and never reacquire it. Start adding back either $g$ or $h$ and see how the filter snaps back to the data. See what the difference in the line is when you add only $g$ vs only $h$. Can you explain the reason for the difference? Then try setting $g$ greater than 1. Can you explain the results? Put $g$ back to a reasonable value (such as 0.1), and then make $h$ very large. Can you explain these results? Finally, set both $g$ and $h$ to their largest values. \n",
    " \n",
    "If you want to explore with this more, change the value of the array `zs` to the values used in any of the charts above and rerun the cell to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "# my FloatSlider returns an ipywidgets.FloatSlider with\n",
    "# continuous_update=False. Filtering code runs too slowly\n",
    "# to instantly react to slider changes.\n",
    "from kf_book.book_plots import FloatSlider\n",
    "\n",
    "zs1 = gen_data(x0=5, dx=5., count=100, noise_factor=50)\n",
    "\n",
    "fig = None\n",
    "def interactive_gh(x, dx, g, h):\n",
    "    global fig\n",
    "    if fig is not None: plt.close(fig)\n",
    "    fig = plt.figure()\n",
    "    data = g_h_filter(data=zs1, x0=x, dx=dx, g=g, h=h)\n",
    "    plt.scatter(range(len(zs1)), zs1, edgecolor='k', \n",
    "                facecolors='none', marker='o', lw=1)\n",
    "    plt.plot(data, color='b')\n",
    "    plt.show()\n",
    "\n",
    "interact(interactive_gh,           \n",
    "         x=FloatSlider(value=0, min=-200, max=200), \n",
    "         dx=FloatSlider(value=5, min=-50, max=50), \n",
    "         g=FloatSlider(value=.1, min=.01, max=2, step=.02), \n",
    "         h=FloatSlider(value=.02, min=.0, max=.5, step=.01));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't Lie to the Filter\n",
    "\n",
    "You are free to set $g$ and $h$ to any value. Here is a filter that performs perfectly despite extreme noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_data(x0=5., dx=.2, count=100, noise_factor=100)\n",
    "data = g_h_filter(data=zs, x0=5., dx=.2, dt=1., g=0., h=0.)\n",
    "\n",
    "book_plots.plot_measurements(zs)\n",
    "book_plots.plot_filter(data, label='filter')\n",
    "plt.legend(loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I brilliantly extracted a straight line out of very noisy data! Maybe I shouldn't try to collect my Fields Medal in mathematics just yet. I did this by setting both $g$ and $h$ to 0. What does this do? It makes the filter ignore the measurements, and so for each update it computes the new position as $x + \\Delta x \\Delta t$. Of course the result is a straight line if we ignore the measurements. \n",
    "\n",
    "A filter that ignores measurements is useless. I know you would never set both $g$ and $h$ to zero as that takes a special kind of genius that only I possess, but I promise that if you are not careful you will set them lower than they should be. You can always make great looking results from test data. When you try your filter on different data you will be disappointed in the results because you finely tuned the constants for a specific data set. $g$ and $h$ must reflect the real world behavior of the system you are filtering, not the behavior of one specific data set. In later chapters we will learn a lot about how to do that. For now I can only say be careful, or you will be getting perfect results with your test data, but results like this once you switch to real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_data(x0=5, dx=-2, count=100, noise_factor=5)\n",
    "data = g_h_filter(data=zs, x0=5., dx=2., dt=1., g=.005, h=0.001)\n",
    "book_plots.plot_measurements(zs)\n",
    "book_plots.plot_filter(data, label='filter')\n",
    "plt.legend(loc=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking a Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready for a practical example. Earlier in the chapter we talked about tracking a train. Trains are heavy and slow, thus they cannot change speed quickly. They are on a track, so they cannot change direction except by slowing to a stop and then reversing course. Hence, we can conclude that if we already know the train's approximate position and velocity then we can predict its position in the near future with a great deal of accuracy. A train cannot change its velocity much in a second or two. \n",
    "\n",
    "So let's write a filter for a train. Its position is expressed as its position on the track in relation to some fixed point which we say is 0 km. I.e., a position of 1 means that the train is 1 km away from the fixed point. Velocity is expressed as meters per second. We perform measurement of position once per second, and the error is $\\pm$ 500 meters. How should we implement our filter?\n",
    "\n",
    "First, let's simulate the situation without a filter. We will assume that the train is currently at kilometer 23, and moving at 15 m/s. We can code this as \n",
    "\n",
    "```python\n",
    "pos = 23*1000\n",
    "vel = 15\n",
    "```\n",
    "\n",
    "Now we can compute the position of the train at some future time, *assuming* no change in velocity, with\n",
    "\n",
    "```python\n",
    "def compute_new_position(pos, vel, dt=1):\n",
    "    return pos + (vel * dt)\n",
    "```\n",
    "\n",
    "We can simulate the measurement by adding in some random noise to the position. Here our error is 500m, so the code might look like:\n",
    "\n",
    "```python\n",
    "def measure_position(pos):\n",
    "        return pos + random.randn()*500\n",
    "```\n",
    "        \n",
    "Let's put that in a cell and plot the results of 100 seconds of simulation. I will use NumPy's `asarray` function to convert the data into an NumPy array. This will allow me to divide all of the elements of the array at once by using the '/' operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "\n",
    "def compute_new_position(pos, vel, dt=1.):\n",
    "    \"\"\" dt is the time delta in seconds.\"\"\"\n",
    "    return pos + (vel * dt)\n",
    "\n",
    "def measure_position(pos):\n",
    "    return pos + randn()*500\n",
    "\n",
    "def gen_train_data(pos, vel, count):\n",
    "    zs = []\n",
    "    for t in range(count):\n",
    "        pos = compute_new_position(pos, vel)\n",
    "        zs.append(measure_position(pos))\n",
    "    return np.asarray(zs)\n",
    "  \n",
    "pos, vel = 23.*1000, 15.\n",
    "zs = gen_train_data(pos, vel, 100)\n",
    "\n",
    "plt.plot(zs / 1000.)  # convert to km\n",
    "book_plots.set_labels('Train Position', 'time(sec)', 'km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the chart how poor the measurements are. No real train could ever move like that. \n",
    "\n",
    "So what should we set $g$ and $h$ to if we want to filter this data? We have not developed the theory for this, but let's try to get a reasonable answer by the seat of our pants. We know that the measurements are very inaccurate, so we don't want to give them much weight at all. To do this we need to choose a very small $g$. We also know that trains can not accelerate or decelerate quickly, so we also want a very small $h$. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zs = gen_train_data(pos=pos, vel=15., count=100)\n",
    "data = g_h_filter(data=zs, x0=pos, dx=15., dt=1., g=.01, h=0.0001)\n",
    "plot_g_h_results(zs/1000., data/1000., 'g=0.01, h=0.0001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty good for an initial guess. Let's make $g$ larger to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_train_data(pos=pos, vel=15., count=100)\n",
    "data = g_h_filter(data=zs, x0=pos, dx=15., dt=1., g=.2, h=0.0001)\n",
    "plot_g_h_results(zs/1000., data/1000., 'g=0.2, h=0.0001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made `g=0.2` and we can see that while the train's position is smoothed, the estimated position (and hence velocity) fluctuates a lot in a very tiny frame, far more than a real train can do. So empirically we know that we want `g<<0.2`.\n",
    "\n",
    "Now let's see the effect of a poor choice for $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_train_data(pos=pos, vel=15., count=100)\n",
    "data = g_h_filter(data=zs, x0=pos, dx=15., dt=1., g=0.01, h=0.1)\n",
    "plot_g_h_results(zs/1000., data/1000., 'g=0.01, h=0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the position changes smoothly thanks to the small $g$, but the large $h$ makes the filter very reactive to the measurements. This happens because in the course of a few seconds the rapidly changing measurement implies a very large velocity change, and a large $h$ tells the filter to react to those changes quickly. Trains cannot change velocity quickly, so the filter is not doing a good job of filtering the data - the filter is changing velocity faster than a train can.\n",
    "\n",
    "Finally, let's add some acceleration to the train. I don't know how fast a train can actually accelerate, but let's say it is accelerating at 0.2 m/sec^2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_data_with_acc(pos, vel, count):\n",
    "    zs = []\n",
    "    for t in range(count):\n",
    "        pos = compute_new_position(pos, vel)\n",
    "        vel += 0.2\n",
    "        zs.append(measure_position(pos))\n",
    "    return np.asarray(zs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_train_data_with_acc(pos=pos, vel=15., count=100)\n",
    "data = g_h_filter(data=zs, x0=pos, dx=15., dt=1., g=.01, h=0.0001)\n",
    "plot_g_h_results(zs/1000., data/1000., 'g=0.01, h=0.0001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the filter is not quite tracking the train anymore due to the acceleration. We can fiddle with $h$ to let it track better, at the expense of a less smooth filtered estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = gen_train_data_with_acc(pos=pos, vel=15., count=100)\n",
    "data = g_h_filter(data=zs, x0=pos, dx=15., dt=1., g=.01, h=0.001)\n",
    "plot_g_h_results(zs/1000., data/1000., 'g=0.01, h=0.001')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two lessons to be learned here. First, use the $h$ term to respond to changes in velocity that you are not modeling. But, far more importantly, there is a trade off here between responding quickly and accurately to changes in behavior and producing ideal output for when the system is in a steady state that you have. If the train never changes velocity we would make $h$ extremely small to avoid having the filtered estimate unduly affected by the noise in the measurement. But in an interesting problem there are almost always changes in state, and we want to react to them quickly. The more quickly we react to them, the more we are affected by the noise in the sensors. \n",
    "\n",
    "I could go on, but my aim is not to develop g-h filter theory here so much as to build insight into how combining measurements and predictions leads to a filtered solution. There is extensive literature on choosing $g$ and $h$ for problems such as this, and there are optimal ways of choosing them to achieve various goals. As I explained earlier it is easy to 'lie' to the filter when experimenting with test data like this. In the subsequent chapters we will learn how the Kalman filter solves this problem in the same basic manner, but with far more sophisticated mathematics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g-h Filters with FilterPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FilterPy](https://github.com/rlabbe/filterpy) is an open source filtering library that I wrote. It has all of the filters in this book, along with others. It is rather easy to program your own g-h filter, but as we progress we will rely on FilterPy more. As a quick introduction, let's look at the g-h filter in FilterPy.\n",
    "\n",
    "If you do not have FilterPy installed just issue the following command from the command line.\n",
    "\n",
    "    pip install filterpy\n",
    "    \n",
    "Read Appendix A for more information on installing or downloading FilterPy from GitHub.\n",
    "\n",
    "To use the g-h filter import it and create an object from the class `GHFilter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from filterpy.gh import GHFilter\n",
    "f = GHFilter(x=0., dx=0., dt=1., g=.8, h=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you construct the object you specify the initial value and rate of change for the signal (`x` and 'dx'), the time step between updates(`dt`) and the two filter parameter (`g` and `h`). `dx` must have the same units of `x`/`dt` - if `x` is in meters and `dt` is in seconds then `dx` must be in meters per second.\n",
    "\n",
    "To run the filter call update, passing the measurement in the parameter `z`, which you'll recall is a standard name for measurements in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.update(z=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`update()` returns the new value of `x` and `dx` in a tuple, but you can also access them from the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.x, f.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can dynamically alter `g` and `h`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.update(z=2.1, g=.85, h=.15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter a sequence of measurements in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.batch_filter([3., 4., 5.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter multiple independent variables. If you are tracking an aircraft you'll need to track it in 3D space. Use NumPy arrays for `x`, `dx`, and the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_0  = np.array([1., 10., 100.])\n",
    "dx_0 = np.array([10., 12., .2])\n",
    "               \n",
    "f_air = GHFilter(x=x_0, dx=dx_0, dt=1., g=.8, h=.2)\n",
    "f_air.update(z=np.array((2., 11., 102.)))\n",
    "print(' x =', f_air.x)\n",
    "print('dx =', f_air.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `GHFilterOrder` allows you to create a filter of order 0, 1, or 2. A g-h filter is order 1. The g-h-k filter, which we haven't talked about, also tracks accelerations. Both classes have functionality required by real applications, such as computing the Variance Reduction Factor (VRF), which we haven't discussed in this chapter. I could fill a book just on the theory and applications of g-h filters, but we have other goals in this book. If you are interested, explore the FilterPy code and do some further reading.\n",
    "\n",
    "The documentation for FilterPy is at https://filterpy.readthedocs.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to experiment with this filter to develop your understanding of how it reacts. It shouldn't take too many attempts to come to the realization that ad-hoc choices for $g$ and $h$ do not perform very well. A particular choice might perform well in one situation, but very poorly in another. Even when you understand the effect of $g$ and $h$ it can be difficult to choose proper values. In fact, it is extremely unlikely that you will choose values for $g$ and $h$ that is optimal for any given problem. Filters are *designed*, not selected *ad hoc*. \n",
    "\n",
    "In some ways I do not want to end the chapter here, as there is a significant amount that we can say about selecting $g$ and $h$. But the g-h filter in this form is not the purpose of this book. Designing the Kalman filter requires you to specify a number of parameters - indirectly they do relate to choosing $g$ and $h$, but you will never refer to them directly when designing Kalman filters. Furthermore, $g$ and $h$ will vary at every time step in a very non-obvious manner. \n",
    "\n",
    "There is another feature of these filters we have barely touched upon - Bayesian statistics. You will note that the term 'Bayesian' is in the title of this book; this is not a coincidence! For the time being we will leave $g$ and $h$ behind, largely unexplored, and develop a very powerful form of probabilistic reasoning about filtering. Yet suddenly this same g-h filter algorithm will appear, this time with a formal mathematical edifice that allows us to create filters from multiple sensors, to accurately estimate the amount of error in our solution, and to control robots."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "10"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "030106778efe4a85a0ddab8fa804a260": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/output",
       "_model_module_version": "1.0.0",
       "_model_name": "OutputModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/output",
       "_view_module_version": "1.0.0",
       "_view_name": "OutputView",
       "layout": "IPY_MODEL_ca9c4841f945412b8a4ddc271fbf7e37",
       "msg_id": "",
       "outputs": []
      }
     },
     "1acb454f11d446f5a5120621e72a9c76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "2bfd983c447245aa9d78aab8d5234275": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "_dom_classes": [
        "widget-interact"
       ],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "VBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "VBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_688b14c67c0849d7a54ff6047a7f18fc",
        "IPY_MODEL_8635a977ba154c9885f0a564858bf4ab",
        "IPY_MODEL_c937c167b8f44990b046fb0b47143a6d",
        "IPY_MODEL_4ca056dce2ea433f86761f5e867deb72",
        "IPY_MODEL_030106778efe4a85a0ddab8fa804a260"
       ],
       "layout": "IPY_MODEL_af5edf21ae2945759353d8ca0351e503"
      }
     },
     "38a69b1a1ddd44608d02d1f273b012f5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46e8517a03cc469ba33d9adbc0ba5b42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4ca056dce2ea433f86761f5e867deb72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "h",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_783a2cb24eb74c20ba57d15fc1e98c85",
       "max": 0.5,
       "min": 0,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.01,
       "style": "IPY_MODEL_6c59aa5c715e4bcb9f5effb09f84d944",
       "value": 0.02
      }
     },
     "688b14c67c0849d7a54ff6047a7f18fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "x",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_79f1d6254838491f838fb1a3cb695c21",
       "max": 200,
       "min": -200,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.1,
       "style": "IPY_MODEL_1acb454f11d446f5a5120621e72a9c76",
       "value": 0
      }
     },
     "6c59aa5c715e4bcb9f5effb09f84d944": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "783a2cb24eb74c20ba57d15fc1e98c85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "79f1d6254838491f838fb1a3cb695c21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ac233c1b7354257b5969fcbf7146daf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "8635a977ba154c9885f0a564858bf4ab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "dx",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_46e8517a03cc469ba33d9adbc0ba5b42",
       "max": 50,
       "min": -50,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.1,
       "style": "IPY_MODEL_92cc136b707c4166bf3a09ab06222a28",
       "value": 5
      }
     },
     "92cc136b707c4166bf3a09ab06222a28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "SliderStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "SliderStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": "",
       "handle_color": null
      }
     },
     "af5edf21ae2945759353d8ca0351e503": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c937c167b8f44990b046fb0b47143a6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatSliderModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatSliderModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "FloatSliderView",
       "continuous_update": false,
       "description": "g",
       "description_tooltip": null,
       "disabled": false,
       "layout": "IPY_MODEL_38a69b1a1ddd44608d02d1f273b012f5",
       "max": 2,
       "min": 0.01,
       "orientation": "horizontal",
       "readout": true,
       "readout_format": ".2f",
       "step": 0.02,
       "style": "IPY_MODEL_7ac233c1b7354257b5969fcbf7146daf",
       "value": 0.1
      }
     },
     "ca9c4841f945412b8a4ddc271fbf7e37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
